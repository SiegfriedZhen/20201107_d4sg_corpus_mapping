{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import datetime\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ckiptagger import data_utils, construct_dictionary, WS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(df, output_file, text_column):\n",
    "    '''\n",
    "    將DataFrame 清洗後輸出 csv。\n",
    "    '''\n",
    "    #trans columns to chinese\n",
    "    df.columns = [ x.split('\\n')[0] for x in df.columns]\n",
    "    # filter NA\n",
    "    df = df[df[text_column].notna()] # 缺失內容非空 (22483, 9)\n",
    "    df = df[df[text_column].apply(lambda x: pd.to_numeric(x, errors='coerce')).isna()] # 缺失內容非單一數字 (22481, 9)\n",
    "    df = df[df['條'].notna()] # 條非空 (15198, 9)\n",
    "    replace_dict = {\n",
    "      '零' : '0'\n",
    "    , '壹' : '1'\n",
    "    , '貳' : '2'\n",
    "    , '參' : '3'\n",
    "    , '肆' : '4'\n",
    "    , '伍' : '5'\n",
    "    , '陸' : '6'\n",
    "    , '柒' : '7'\n",
    "    , '捌' : '8'\n",
    "    , '玖' : '9'\n",
    "    , '一' : '1'\n",
    "    , '二' : '2'\n",
    "    , '三' : '3'\n",
    "    , '四' : '4'\n",
    "    , '五' : '5'\n",
    "    , '六' : '6'\n",
    "    , '七' : '7'\n",
    "    , '八' : '8'\n",
    "    , '九' : '9'\n",
    "    , '拾' : '10'\n",
    "    , '佰' : '100'\n",
    "    , '十' : '10' \n",
    "    , '兩' : '2'\n",
    "    , '倆' : '2'\n",
    "    }\n",
    "    df.replace({parsed_column: replace_dict}, regex=True, inplace=True)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(output_file, ' exported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punctuation(file_path, output_file, text_column):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    punc = list()\n",
    "    for i, s in enumerate(df[text_column]):\n",
    "        s = re.findall(r'[^\\u4e00-\\u9fff]+', s)\n",
    "        s = ''.join(s)\n",
    "        s = re.findall(r'[^\\w]+', s)\n",
    "        s = ''.join(s)\n",
    "        punc = punc + list(s)\n",
    "    \n",
    "    punc = set(punc)\n",
    "    punc = punc.union(punctuation).union({'\\n' , '\\t' , '【' , '】' , '「' , '」' , '.'  , '。' })\n",
    "    pickle.dump(punc, open(output_file, 'wb'))\n",
    "    print(output_file, ' exported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cut(file_path, output_file, punc_pkl, text_column, legal_name_file, word_file):\n",
    "    word_to_weight = {}\n",
    "    with open(legal_name_file, 'r', encoding='big5') as k1, open(word_file, 'r') as k2:\n",
    "        k = k1.read().split('\\n') + k2.read().split('\\n') \n",
    "        word_to_weight = dict([(_, 1) for _ in k])\n",
    "    dictionary = construct_dictionary(word_to_weight)\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    punc = pickle.load(open(punc_pkl, 'rb'))\n",
    "    word_s = ws(df[text_column], sentence_segmentation=True, segment_delimiter_set=punc)\n",
    "    word_s1 = [ [_ for _ in w if _ not in punctuation]   for w in word_s ]\n",
    "    df['token'] =['@'.join(_) for _ in word_s1]\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(output_file, ' exported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_modle(file_path, output_file):\n",
    "    model_var = {}\n",
    "    df = pd.read_csv(file_path)\n",
    "    #, 'data_ETL3noPuncDict.csv'\n",
    "\n",
    "    # Replace '@' with ' ' in original dataframe\n",
    "    df.token = df.token.apply(lambda text: text.replace('@', ' '))\n",
    "\n",
    "    tfidf_ml = TfidfVectorizer()\n",
    "    tfidf_ml.fit(df.token)\n",
    "\n",
    "    # TF-IDF Dicitonary\n",
    "    dictionary = dict(zip(tfidf_ml.get_feature_names(), list(tfidf_ml.idf_)))\n",
    "\n",
    "    # feature name\n",
    "    tfidf_feature = tfidf_ml.get_feature_names()\n",
    "\n",
    "    w2v_model = Word2Vec(df.token.apply(lambda text: text.split()))\n",
    "    w2v_vocab = list(w2v_model.wv.vocab)\n",
    "    print(w2v_model)\n",
    "\n",
    "    starttime = datetime.datetime.now()\n",
    "\n",
    "    # TF-IDF weighted Word2Vec\n",
    "    tfidf_text_vect = [] # tfidf-w2v is stored in this list\n",
    "    row = 0\n",
    "\n",
    "    for text in df.token.apply(lambda text: text.split()):\n",
    "        text_vect = np.zeros(100)\n",
    "        weight_sum = 0\n",
    "        for word in text:\n",
    "            if word in w2v_vocab and word in tfidf_feature:\n",
    "                vec = w2v_model.wv[word]\n",
    "                tf_idf = dictionary[word]*(text.count(word)/len(text))\n",
    "                text_vect += (vec * tf_idf)\n",
    "                weight_sum += tf_idf\n",
    "        if weight_sum != 0:\n",
    "            text_vect /= weight_sum\n",
    "        tfidf_text_vect.append(text_vect)\n",
    "        row += 1\n",
    "\n",
    "    # calculate running time\n",
    "    endtime = datetime.datetime.now()\n",
    "    print(\"建立模型時間: \",endtime - starttime)\n",
    "    model_var = [w2v_vocab, w2v_model, tfidf_feature, tfidf_text_vect, dictionary, df]\n",
    "    pickle.dump(model_var, open(output_file, 'wb'))\n",
    "    print(output_file, ' exported.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
